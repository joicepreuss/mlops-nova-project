{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, Tuple, Any, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nannyml as nml\n",
    "from nannyml.drift.multivariate.data_reconstruction.result import Result\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def check_data_drift(reference : pd.DataFrame, analysis : pd.DataFrame, parameters : Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Data drift detection.\n",
    "    - Multivariate data drift\n",
    "    - Univariate data drift\n",
    "    - Model performance drift\n",
    "\n",
    "    Args:\n",
    "    --\n",
    "        reference (pd.DataFrame): Reference dataset\n",
    "        analysis (pd.DataFrame): Analysis dataset\n",
    "        parameters (Dict[str, Any]): Parameters\n",
    "\n",
    "    \"\"\"\n",
    "    reference = create_timestamp_column(reference, \n",
    "                                        column_name_year=\"YrSold\", \n",
    "                                        column_name_month=\"MoSold\")\n",
    "    \n",
    "    analysis = create_timestamp_column(analysis, \n",
    "                                       column_name_year=\"YrSold\", \n",
    "                                       column_name_month=\"MoSold\")\n",
    "    \n",
    "    feature_columns = parameters[\"most_important_features\"]\n",
    "\n",
    "    reference = reference[feature_columns + [\"timestamp\", \"y_pred\", \"y_true\"]]\n",
    "    analysis = analysis[feature_columns + [\"timestamp\"]]\n",
    "    \n",
    "    calculate_drift_multivariat(reference, \n",
    "                                 analysis, \n",
    "                                 feature_column_names=feature_columns,\n",
    "                                 timestamp_column_name=\"timestamp\")\n",
    "    \n",
    "    calculate_drift_univariate(reference, \n",
    "                                analysis, \n",
    "                                column_names=feature_columns, \n",
    "                                treat_as_categorical=[], \n",
    "                                timestamp_column_name=\"timestamp\")\n",
    "    \n",
    "    estimate_performance(reference,\n",
    "                        analysis,\n",
    "                        feature_column_names=feature_columns,\n",
    "                        y_pred=reference[\"y_pred\"],\n",
    "                        y_true=reference[\"y_true\"],\n",
    "                        timestamp_column_name=\"timestamp\",\n",
    "                        metrics=['rmse', 'rmsle'],\n",
    "                        tune_hyperparameters=False)\n",
    "    \n",
    "    create_psi_plot(feature_columns, reference, analysis)\n",
    "\n",
    "\n",
    "\n",
    "def create_timestamp_column(df : pd.DataFrame, column_name_year : str, column_name_month : str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function creates a new timestamp column using a passed year and month column.\n",
    "    \"\"\"\n",
    "    df['timestamp'] = pd.to_datetime(df[column_name_year].astype(str) + '-' + df[column_name_month].astype(str), format='%Y-%m')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_drift_multivariat(reference : pd.DataFrame, analysis : pd.DataFrame,\n",
    "                                 feature_column_names : List[str], timestamp_column_name : str=\"timestamp\") -> None:\n",
    "    \"\"\"\n",
    "    Calculates and plots the multivariant data drift.\n",
    "\n",
    "    Args:\n",
    "    --\n",
    "        reference (pd.DataFrame): Reference dataset\n",
    "        analysis (pd.DataFrame): Analysis dataset\n",
    "        feature_column_names (List[str]): List of feature column names\n",
    "        timestamp_column_name (str): Timestamp column name\n",
    "    \"\"\"\n",
    "\n",
    "    folder_path = '../data/08_reporting/Data_drifts_reporting/Multivariate_drifts'\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    calc = nml.DataReconstructionDriftCalculator(column_names=feature_column_names,\n",
    "                                                 timestamp_column_name=timestamp_column_name\n",
    "                                                 )\n",
    "    calc.fit(reference)\n",
    "\n",
    "    results = calc.calculate(analysis)\n",
    "    analysis_results = results.filter(period='analysis').to_df()\n",
    "    reference_results = results.filter(period='reference').to_df()\n",
    "\n",
    "    analysis_results.to_csv(os.path.join(folder_path, 'Multivariate_analysis_results.csv'))\n",
    "    reference_results.to_csv(os.path.join(folder_path, 'Multivariate_reference_results.csv'))\n",
    "\n",
    "    figure = results.plot()\n",
    "    file_path = os.path.join(folder_path, 'multivariate_drift.html')\n",
    "    figure.write_html(file_path)\n",
    "\n",
    "\n",
    "def calculate_drift_univariate(reference : pd.DataFrame, analysis : pd.DataFrame,\n",
    "                                column_names : List[str], treat_as_categorical : List[str],\n",
    "                                timestamp_column_name : str, continuous_methods : List[str]=['kolmogorov_smirnov', 'jensen_shannon'],\n",
    "                                categorical_methods : List[str]=['chi2', 'jensen_shannon']) -> Result:\n",
    "    \"\"\"\n",
    "    Calculates and plots the univariate data drift.\n",
    "    The used methods are:\n",
    "    - Continuous: Kolmogorov-Smirnov, Jensen-Shannon\n",
    "    - Categorical: Chi2, Jensen-Shannon\n",
    "\n",
    "    Args:\n",
    "    --\n",
    "        reference (pd.DataFrame): Reference dataset\n",
    "        analysis (pd.DataFrame): Analysis dataset\n",
    "        column_names (List[str]): List of column names\n",
    "        treat_as_categorical (List[str]): List of column names to treat as categorical\n",
    "        timestamp_column_name (str): Timestamp column name\n",
    "        continuous_methods (List[str]): List of continuous methods\n",
    "        categorical_methods (List[str]): List of categorical methods\n",
    "    \"\"\"\n",
    "\n",
    "    folder_path = '../data/08_reporting/Data_drifts_reporting/Univariate_drifts'\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    calc = nml.UnivariateDriftCalculator(column_names=column_names,\n",
    "                                         treat_as_categorical=treat_as_categorical,\n",
    "                                         timestamp_column_name=timestamp_column_name,\n",
    "                                         continuous_methods=continuous_methods,\n",
    "                                         categorical_methods=categorical_methods\n",
    "                                         )\n",
    "    calc.fit(reference)\n",
    "    results = calc.calculate(analysis)\n",
    "\n",
    "    analysis_results = results.filter(period='analysis').to_df()\n",
    "    reference_results = results.filter(period='reference').to_df()\n",
    "\n",
    "    analysis_results.to_csv(os.path.join(folder_path, 'Univariate_analysis_results.csv'))\n",
    "    reference_results.to_csv(os.path.join(folder_path, 'Univariate_reference_results.csv'))\n",
    "\n",
    "    jensen = results.filter(column_names=results.continuous_column_names, methods=['jensen_shannon']).plot(kind='drift')\n",
    "    file_path_jensen = os.path.join(folder_path, 'Univariate_drift_jensen_shannon.html')\n",
    "    jensen.write_html(file_path_jensen)\n",
    "    \n",
    "    kolgomorov = results.filter(column_names=results.continuous_column_names, methods=['kolmogorov_smirnov']).plot(kind='drift')\n",
    "    file_path_kolgomorov = os.path.join(folder_path, 'Univariate_drift_kolgomorov_smirnov.html')\n",
    "    kolgomorov.write_html(file_path_kolgomorov)\n",
    "\n",
    "\n",
    "\n",
    "def estimate_performance(reference : pd.DataFrame, \n",
    "                         analysis : pd.DataFrame,\n",
    "                         feature_column_names : List[str], \n",
    "                         y_pred : pd.Series, y_true : pd.Series,\n",
    "                         timestamp_column_name : str, \n",
    "                         metrics : str =\"mse\",\n",
    "                         tune_hyperparameters = False): \n",
    "    \"\"\"\n",
    "    Estimates the model performance using the DLE algorithm from NannyML.\n",
    "\n",
    "    Args:\n",
    "    --\n",
    "        reference (pd.DataFrame): Reference dataset\n",
    "        analysis (pd.DataFrame): Analysis dataset\n",
    "        feature_column_names (List[str]): List of feature column names\n",
    "        y_pred (pd.Series): Predicted target values\n",
    "        y_true (pd.Series): True target values\n",
    "        timestamp_column_name (str): Timestamp column name\n",
    "        metrics (str): Metric to use for performance estimation\n",
    "        tune_hyperparameters (bool): Whether to tune the hyperparameters\n",
    "    \"\"\"\n",
    "    folder_path = '../data/08_reporting/Data_drifts_reporting/Estimate_performance'\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    estimator = nml.DLE(feature_column_names=feature_column_names,\n",
    "                        y_pred=y_pred,\n",
    "                        y_true=y_true,\n",
    "                        timestamp_column_name=timestamp_column_name,\n",
    "                        metrics=metrics,\n",
    "                        tune_hyperparameters=tune_hyperparameters\n",
    "                        )\n",
    "    \n",
    "    estimator.fit(reference)\n",
    "    results = estimator.estimate(analysis)\n",
    "\n",
    "    analysis_results = results.filter(period='analysis').to_df()\n",
    "    reference_results = results.filter(period='reference').to_df()\n",
    "    analysis_results.to_csv(os.path.join(folder_path, 'Estimate_performance_analysis_results.csv'))\n",
    "    reference_results.to_csv(os.path.join(folder_path, 'Estimate_performance_reference_results.csv'))\n",
    "\n",
    "    metric_fig = results.plot()\n",
    "    file_path = os.path.join(folder_path, 'estimate_performance.html')\n",
    "    metric_fig.write_html(file_path)\n",
    "\n",
    "\n",
    "# CODE FOR PSI FROM LAB1\n",
    "\n",
    "def calculate_psi(expected, actual, buckettype='bins', buckets=10, axis=0):\n",
    "    '''\n",
    "    Code copied from the Practical Lab for data drift from MLOps course.\n",
    "    Calculate the PSI (population stability index) across all variables.\n",
    "    Args:\n",
    "       expected: numpy matrix of original values\n",
    "       actual: numpy matrix of new values, same size as expected\n",
    "       buckettype: type of strategy for creating buckets, bins splits into even splits, quantiles splits into quantile buckets\n",
    "       buckets: number of quantiles to use in bucketing variables\n",
    "       axis: axis by which variables are defined, 0 for vertical, 1 for horizontal\n",
    "    Returns:\n",
    "       psi_values: ndarray of psi values for each variable\n",
    "    Author:\n",
    "       Matthew Burke\n",
    "       github.com/mwburke\n",
    "       worksofchart.com\n",
    "    '''\n",
    "\n",
    "    def psi(expected_array, actual_array, buckets):\n",
    "        '''Calculate the PSI for a single variable\n",
    "        Args:\n",
    "           expected_array: numpy array of original values\n",
    "           actual_array: numpy array of new values, same size as expected\n",
    "           buckets: number of percentile ranges to bucket the values into\n",
    "        Returns:\n",
    "           psi_value: calculated PSI value\n",
    "        '''\n",
    "\n",
    "        def scale_range (input, min, max):\n",
    "            input += -(np.min(input))\n",
    "            input /= np.max(input) / (max - min)\n",
    "            input += min\n",
    "            return input\n",
    "\n",
    "        breakpoints = np.arange(0, buckets + 1) / (buckets) * 100\n",
    "\n",
    "        if buckettype == 'bins':\n",
    "            breakpoints = scale_range(breakpoints, np.min(expected_array), np.max(expected_array))\n",
    "        elif buckettype == 'quantiles':\n",
    "            breakpoints = np.stack([np.percentile(expected_array, b) for b in breakpoints])\n",
    "\n",
    "        expected_percents = np.histogram(expected_array, breakpoints)[0] / len(expected_array)\n",
    "        actual_percents = np.histogram(actual_array, breakpoints)[0] / len(actual_array)\n",
    "\n",
    "        def sub_psi(e_perc, a_perc):\n",
    "            '''Calculate the actual PSI value from comparing the values.\n",
    "               Update the actual value to a very small number if equal to zero\n",
    "            '''\n",
    "            if a_perc == 0:\n",
    "                a_perc = 0.0001\n",
    "            if e_perc == 0:\n",
    "                e_perc = 0.0001\n",
    "\n",
    "            value = (e_perc - a_perc) * np.log(e_perc / a_perc)\n",
    "            return(value)\n",
    "\n",
    "        psi_value = np.sum(sub_psi(expected_percents[i], actual_percents[i]) for i in range(0, len(expected_percents)))\n",
    "\n",
    "        return(psi_value)\n",
    "\n",
    "    if len(expected.shape) == 1:\n",
    "        psi_values = np.empty(len(expected.shape))\n",
    "    else:\n",
    "        psi_values = np.empty(expected.shape[axis])\n",
    "\n",
    "    for i in range(0, len(psi_values)):\n",
    "        if len(psi_values) == 1:\n",
    "            psi_values = psi(expected, actual, buckets)\n",
    "        elif axis == 0:\n",
    "            psi_values[i] = psi(expected[:,i], actual[:,i], buckets)\n",
    "        elif axis == 1:\n",
    "            psi_values[i] = psi(expected[i,:], actual[i,:], buckets)\n",
    "\n",
    "    return(psi_values)\n",
    "\n",
    "def create_psi_plot(numerical_features, reference, analysis):\n",
    "    \"\"\"\n",
    "    Create a plot of the PSI values for each numerical feature\n",
    "    \"\"\"\n",
    "    folder_path = '../data/08_reporting/Data_drifts_reporting'\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    psis_num = []\n",
    "\n",
    "    #Using the github implementation to compute PSI's numerical features\n",
    "    for feature_name in numerical_features:\n",
    "        psi = calculate_psi(reference[feature_name], analysis[feature_name], buckettype='bins', buckets=20, axis=0)\n",
    "        psis_num.append(psi)\n",
    "    #Plot each feature's PSI value\n",
    "    height = psis_num\n",
    "    bars = numerical_features\n",
    "    y_pos = np.arange(len(bars))\n",
    "    plt.barh(y_pos, height)\n",
    "    plt.axvline(x=0.2,color='red')\n",
    "    plt.yticks(y_pos, bars)\n",
    "    plt.xlabel(\"PSI\")\n",
    "    plt.title(\"PSI for numerical features\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    file_path = os.path.join(folder_path, 'psi_numerical_features.png')\n",
    "    plt.savefig(file_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"most_important_features\": [\"OverallQual\", \"GrLivArea\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = pd.read_csv('../data/03_primary/X_to_predict.csv')\n",
    "y_train = pd.read_csv('../data/05_model_input/y_train.csv')\n",
    "predictions = pd.read_csv('../data/07_model_output/df_with_predict.csv')[\"y_pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge reference with y_train and predictions on index of reference\n",
    "reference = reference.merge(y_train, left_index=True, right_index=True)\n",
    "reference = reference.merge(predictions, left_index=True, right_index=True)\n",
    "# rename SalePrice to y_true\n",
    "reference = reference.rename(columns={\"SalePrice\": \"y_true\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "analysis = reference.copy()\n",
    "\n",
    "analysis['timestamp'] = pd.to_datetime(analysis['timestamp'])  # Convert timestamp column to datetime format\n",
    "\n",
    "analysis.loc[(analysis[\"OverallQual\"] > 10) & (analysis[\"timestamp\"] > pd.to_datetime(\"2009-01-01\")), \"OverallQual\"] = random.randint(9, 20)\n",
    "analysis.loc[(analysis[\"GrLivArea\"] > 1000) & (analysis[\"timestamp\"] > pd.to_datetime(\"2009-01-01\")), \"GrLivArea\"] = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save reference and analysis to csv in data/09_data_drift_test\n",
    "folder_path = '../data/09_data_drift_test'\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "reference.to_csv(os.path.join(folder_path, 'reference.csv'), index=False)\n",
    "analysis.to_csv(os.path.join(folder_path, 'analysis.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\site-packages\\nannyml\\base.py:299: FutureWarning:\n",
      "\n",
      "The behavior of indexing on a MultiIndex with a nested sequence of labels is deprecated and will change in a future version. `series.loc[label, sequence]` will raise if any members of 'sequence' or not present in the index's second level. To retain the old behavior, use `series.index.isin(sequence, level=1)`\n",
      "\n"
     ]
    },
    {
     "ename": "CalculatorException",
     "evalue": "failed while fitting DLE[tune_hyperparameters=False, metrics=['RMSE', 'RMSLE']].\nThe truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\site-packages\\nannyml\\usage_logging.py:208\u001b[0m, in \u001b[0;36mlog_usage.<locals>.logging_decorator.<locals>.logging_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[39m# log the event\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     logger\u001b[39m.\u001b[39;49mlog(usage_event, md)\n\u001b[0;32m    209\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\site-packages\\nannyml\\usage_logging.py:102\u001b[0m, in \u001b[0;36mUsageLogger.log\u001b[1;34m(self, usage_event, metadata)\u001b[0m\n\u001b[0;32m    101\u001b[0m     metadata \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 102\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log(usage_event, metadata)\n\u001b[0;32m    103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlogged usage for event \u001b[39m\u001b[39m{\u001b[39;00musage_event\u001b[39m}\u001b[39;00m\u001b[39m with metadata \u001b[39m\u001b[39m{\u001b[39;00mmetadata\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\site-packages\\nannyml\\usage_logging.py:132\u001b[0m, in \u001b[0;36mSegmentUsageTracker._log\u001b[1;34m(self, usage_event, metadata)\u001b[0m\n\u001b[0;32m    131\u001b[0m user_id \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(uuid\u001b[39m.\u001b[39mUUID(\u001b[39mint\u001b[39m\u001b[39m=\u001b[39muuid\u001b[39m.\u001b[39mgetnode()))\n\u001b[1;32m--> 132\u001b[0m metadata\u001b[39m.\u001b[39mupdate(_get_system_information())\n\u001b[0;32m    133\u001b[0m segment_analytics\u001b[39m.\u001b[39mtrack(user_id, usage_event\u001b[39m.\u001b[39mvalue, metadata)\n",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\site-packages\\nannyml\\usage_logging.py:223\u001b[0m, in \u001b[0;36m_get_system_information\u001b[1;34m()\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_system_information\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m    221\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m    222\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mos_type\u001b[39m\u001b[39m\"\u001b[39m: platform\u001b[39m.\u001b[39msystem(),\n\u001b[1;32m--> 223\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mruntime_environment\u001b[39m\u001b[39m\"\u001b[39m: _get_runtime_environment(),\n\u001b[0;32m    224\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpython_version\u001b[39m\u001b[39m\"\u001b[39m: platform\u001b[39m.\u001b[39mpython_version(),\n\u001b[0;32m    225\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnannyml_version\u001b[39m\u001b[39m\"\u001b[39m: __version__,\n\u001b[0;32m    226\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\site-packages\\nannyml\\usage_logging.py:230\u001b[0m, in \u001b[0;36m_get_runtime_environment\u001b[1;34m()\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_runtime_environment\u001b[39m():\n\u001b[1;32m--> 230\u001b[0m     \u001b[39mif\u001b[39;00m _is_running_in_docker():\n\u001b[0;32m    231\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mdocker\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\site-packages\\nannyml\\usage_logging.py:243\u001b[0m, in \u001b[0;36m_is_running_in_docker\u001b[1;34m()\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdocker\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m line \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m Path(\u001b[39m'\u001b[39;49m\u001b[39m/proc/self/cgroup\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mopen()):\n\u001b[0;32m    244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\pathlib.py:1252\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m \u001b[39mOpen the file pointed by this path and return a file object, as\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \u001b[39mthe built-in open() function does.\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m io\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m, mode, buffering, encoding, errors, newline,\n\u001b[0;32m   1253\u001b[0m                opener\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_opener)\n",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\pathlib.py:1120\u001b[0m, in \u001b[0;36mPath._opener\u001b[1;34m(self, name, flags, mode)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_opener\u001b[39m(\u001b[39mself\u001b[39m, name, flags, mode\u001b[39m=\u001b[39m\u001b[39m0o666\u001b[39m):\n\u001b[0;32m   1119\u001b[0m     \u001b[39m# A stub for the opener argument to built-in open()\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m, flags, mode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\proc\\\\self\\\\cgroup'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\site-packages\\nannyml\\base.py:496\u001b[0m, in \u001b[0;36mAbstractEstimator.fit\u001b[1;34m(self, reference_data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    495\u001b[0m     reference_data \u001b[39m=\u001b[39m reference_data\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m--> 496\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(reference_data, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    497\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidArgumentsException:\n",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\site-packages\\nannyml\\usage_logging.py:211\u001b[0m, in \u001b[0;36mlog_usage.<locals>.logging_decorator.<locals>.logging_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39mif\u001b[39;00m runtime_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 211\u001b[0m     \u001b[39mraise\u001b[39;00m runtime_exception\n\u001b[0;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\site-packages\\nannyml\\usage_logging.py:160\u001b[0m, in \u001b[0;36mlog_usage.<locals>.logging_decorator.<locals>.logging_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m     \u001b[39m# run original function\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m     res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    161\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\site-packages\\nannyml\\performance_estimation\\direct_loss_estimation\\dle.py:281\u001b[0m, in \u001b[0;36mDLE._fit\u001b[1;34m(self, reference_data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidArgumentsException(\u001b[39m'\u001b[39m\u001b[39mdata contains no rows. Please provide a valid data set.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 281\u001b[0m _list_missing([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_true, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_pred], \u001b[39mlist\u001b[39;49m(reference_data\u001b[39m.\u001b[39;49mcolumns))\n\u001b[0;32m    283\u001b[0m _, categorical_feature_columns \u001b[39m=\u001b[39m _split_features_by_type(reference_data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_column_names)\n",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\site-packages\\nannyml\\base.py:568\u001b[0m, in \u001b[0;36m_list_missing\u001b[1;34m(columns_to_find, dataset_columns)\u001b[0m\n\u001b[0;32m    566\u001b[0m     dataset_columns \u001b[39m=\u001b[39m dataset_columns\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m--> 568\u001b[0m missing \u001b[39m=\u001b[39m [col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m columns_to_find \u001b[39mif\u001b[39;00m col \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m dataset_columns]\n\u001b[0;32m    569\u001b[0m \u001b[39mif\u001b[39;00m missing:\n",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\site-packages\\nannyml\\base.py:568\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    566\u001b[0m     dataset_columns \u001b[39m=\u001b[39m dataset_columns\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m--> 568\u001b[0m missing \u001b[39m=\u001b[39m [col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m columns_to_find \u001b[39mif\u001b[39;00m col \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m dataset_columns]\n\u001b[0;32m    569\u001b[0m \u001b[39mif\u001b[39;00m missing:\n",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\site-packages\\pandas\\core\\generic.py:1527\u001b[0m, in \u001b[0;36mNDFrame.__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   1526\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__nonzero__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NoReturn:\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1528\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe truth value of a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m is ambiguous. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1529\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1530\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCalculatorException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m check_data_drift(reference, analysis, parameters)\n",
      "Cell \u001b[1;32mIn[37], line 51\u001b[0m, in \u001b[0;36mcheck_data_drift\u001b[1;34m(reference, analysis, parameters)\u001b[0m\n\u001b[0;32m     40\u001b[0m calculate_drift_multivariat(reference, \n\u001b[0;32m     41\u001b[0m                              analysis, \n\u001b[0;32m     42\u001b[0m                              feature_column_names\u001b[39m=\u001b[39mfeature_columns,\n\u001b[0;32m     43\u001b[0m                              timestamp_column_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m calculate_drift_univariate(reference, \n\u001b[0;32m     46\u001b[0m                             analysis, \n\u001b[0;32m     47\u001b[0m                             column_names\u001b[39m=\u001b[39mfeature_columns, \n\u001b[0;32m     48\u001b[0m                             treat_as_categorical\u001b[39m=\u001b[39m[], \n\u001b[0;32m     49\u001b[0m                             timestamp_column_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m estimate_performance(reference,\n\u001b[0;32m     52\u001b[0m                     analysis,\n\u001b[0;32m     53\u001b[0m                     feature_column_names\u001b[39m=\u001b[39;49mfeature_columns,\n\u001b[0;32m     54\u001b[0m                     y_pred\u001b[39m=\u001b[39;49mreference[\u001b[39m\"\u001b[39;49m\u001b[39my_pred\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     55\u001b[0m                     y_true\u001b[39m=\u001b[39;49mreference[\u001b[39m\"\u001b[39;49m\u001b[39my_true\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     56\u001b[0m                     timestamp_column_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     57\u001b[0m                     metrics\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mrmse\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrmsle\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     58\u001b[0m                     tune_hyperparameters\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     60\u001b[0m create_psi_plot(feature_columns, reference, analysis)\n",
      "Cell \u001b[1;32mIn[37], line 187\u001b[0m, in \u001b[0;36mestimate_performance\u001b[1;34m(reference, analysis, feature_column_names, y_pred, y_true, timestamp_column_name, metrics, tune_hyperparameters)\u001b[0m\n\u001b[0;32m    177\u001b[0m os\u001b[39m.\u001b[39mmakedirs(folder_path, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    179\u001b[0m estimator \u001b[39m=\u001b[39m nml\u001b[39m.\u001b[39mDLE(feature_column_names\u001b[39m=\u001b[39mfeature_column_names,\n\u001b[0;32m    180\u001b[0m                     y_pred\u001b[39m=\u001b[39my_pred,\n\u001b[0;32m    181\u001b[0m                     y_true\u001b[39m=\u001b[39my_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m                     tune_hyperparameters\u001b[39m=\u001b[39mtune_hyperparameters\n\u001b[0;32m    185\u001b[0m                     )\n\u001b[1;32m--> 187\u001b[0m estimator\u001b[39m.\u001b[39;49mfit(reference)\n\u001b[0;32m    188\u001b[0m results \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mestimate(analysis)\n\u001b[0;32m    190\u001b[0m analysis_results \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mfilter(period\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39manalysis\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto_df()\n",
      "File \u001b[1;32mc:\\Users\\jkick\\.conda\\envs\\housepricing\\lib\\site-packages\\nannyml\\base.py:502\u001b[0m, in \u001b[0;36mAbstractEstimator.fit\u001b[1;34m(self, reference_data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    500\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m--> 502\u001b[0m     \u001b[39mraise\u001b[39;00m CalculatorException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfailed while fitting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mCalculatorException\u001b[0m: failed while fitting DLE[tune_hyperparameters=False, metrics=['RMSE', 'RMSLE']].\nThe truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "check_data_drift(reference, analysis, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "housepricing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
